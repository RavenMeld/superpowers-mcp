# huggingface-transformers

Hugging Face Transformers best practices including model loading, tokenization, fine-tuning workflows, and inference optimization. Use when working with transformer models, fine-tuning LLMs, implementing NLP tasks, or optimizing transformer inference.

## Quick Facts
- id: `huggingface-transformers--5fff8c380d`
- worth_using_score: `50/100`
- tags: `git, python, ci, docs, rag, llm, eval, benchmark`
- source: `codex_skills`
- source_path: `/home/wolvend/.codex/skills/applied-artificial-intelligence-huggingface-transformers/SKILL.md`

## Workflow / Steps
- ### Pattern 1: Simple Fine-Tuning with Trainer
- ```python
- from transformers import (
- AutoModelForSequenceClassification,
- AutoTokenizer,
- Trainer,

## Signal Summary
- has_description: `True`
- has_use_when: `False`
- has_workflow: `True`
- code_examples: `30`
- has_scripts: `False`
- has_references: `False`
- has_assets: `False`
