# llm-evaluation

Implement comprehensive evaluation strategies for LLM applications using automated metrics, human feedback, and benchmarking. Use when testing LLM performance, measuring AI application quality, or establishing evaluation frameworks.

## Quick Facts
- id: `llm-evaluation--54d98e81df`
- worth_using_score: `60/100`
- tags: `python, go, testing, ci, docs, rag, llm, eval, benchmark`
- source: `agents_skills`
- source_path: `/home/wolvend/.agents/skills/wshobson-llm-evaluation/SKILL.md`

## Use When
- Measuring LLM application performance systematically
- Comparing different models or prompts
- Detecting performance regressions before deployment
- Validating improvements from prompt changes
- Building confidence in production systems
- Establishing baselines and tracking progress over time
- Debugging unexpected model behavior

## Signal Summary
- has_description: `True`
- has_use_when: `True`
- has_workflow: `False`
- code_examples: `14`
- has_scripts: `False`
- has_references: `False`
- has_assets: `False`
