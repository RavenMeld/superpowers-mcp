# phoenix-observability

Open-source AI observability platform for LLM tracing, evaluation, and monitoring. Use when debugging LLM applications with detailed traces, running evaluations on datasets, or monitoring production AI systems with real-time insights.

## Quick Facts
- id: `phoenix-observability--42f11b32d8`
- worth_using_score: `65/100`
- tags: `github, git, python, sql, sqlite, postgres, go, docker, testing, ci, docs, observability, rag, llm, eval`
- source: `agents_skills`
- source_path: `/home/wolvend/.agents/skills/davila7-phoenix-observability/SKILL.md`

## Use When
- *Use Phoenix when:**
- Debugging LLM application issues with detailed traces
- Running systematic evaluations on datasets
- Monitoring production LLM systems in real-time
- Building experiment pipelines for prompt/model comparison
- Self-hosted observability without vendor lock-in
- *Key features:**
- **Tracing**: OpenTelemetry-based trace collection for any LLM framework
- **Evaluation**: LLM-as-judge evaluators for quality assessment
- **Datasets**: Versioned test sets for regression testing

## Signal Summary
- has_description: `True`
- has_use_when: `True`
- has_workflow: `False`
- code_examples: `24`
- has_scripts: `False`
- has_references: `True`
- has_assets: `False`
