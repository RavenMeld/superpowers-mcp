# llm-evaluation

Implement comprehensive evaluation strategies for LLM applications using automated metrics, human feedback, and benchmarking. Use when testing LLM performance, measuring AI application quality, or establishing evaluation frameworks.

## Quick Facts
- id: `llm-evaluation--0778eb1116`
- worth_using_score: `60/100`
- tags: `python, go, testing, ci, rag, llm, eval, benchmark`
- source: `agents_skills`
- source_path: `/home/wolvend/.agents/skills/sickn33-llm-evaluation/SKILL.md`

## Use When
- The task is unrelated to llm evaluation
- You need a different domain or tool outside this scope

## Signal Summary
- has_description: `True`
- has_use_when: `True`
- has_workflow: `False`
- code_examples: `12`
- has_scripts: `False`
- has_references: `False`
- has_assets: `False`
