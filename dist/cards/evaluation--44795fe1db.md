# evaluation

This skill should be used when the user asks to "evaluate agent performance", "build test framework", "measure agent quality", "create evaluation rubrics", or mentions LLM-as-judge, multi-dimensional evaluation, agent testing, or quality gates for agent pipelines.

## Quick Facts
- id: `evaluation--44795fe1db`
- worth_using_score: `50/100`
- tags: `python, go, testing, ci, windows, rag, llm, eval, benchmark`
- source: `agents_skills`
- source_path: `/home/wolvend/.agents/skills/muratcankoylan-evaluation/SKILL.md`

## Signal Summary
- has_description: `True`
- has_use_when: `False`
- has_workflow: `False`
- code_examples: `2`
- has_scripts: `True`
- has_references: `True`
- has_assets: `False`
