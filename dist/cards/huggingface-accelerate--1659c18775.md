# huggingface-accelerate

Simplest distributed training API. 4 lines to add distributed support to any PyTorch script. Unified API for DeepSpeed/FSDP/Megatron/DDP. Automatic device placement, mixed precision (FP16/BF16/FP8). Interactive config, single launch command. HuggingFace ecosystem standard.

## Quick Facts
- id: `huggingface-accelerate--1659c18775`
- worth_using_score: `75/100`
- tags: `github, python, node, ci, docs`
- source: `codex_skills`
- source_path: `/home/wolvend/.codex/skills/davila7-huggingface-accelerate/SKILL.md`

## Use When
- *Use Accelerate when**:
- Want simplest distributed training
- Need single script for any hardware
- Use HuggingFace ecosystem
- Want flexibility (DDP/DeepSpeed/FSDP/Megatron)
- Need quick prototyping
- *Key advantages**:
- **4 lines**: Minimal code changes
- **Unified API**: Same code for DDP, DeepSpeed, FSDP, Megatron
- **Automatic**: Device placement, mixed precision, sharding

## Workflow / Steps
- *Original script**:

## Signal Summary
- has_description: `True`
- has_use_when: `True`
- has_workflow: `True`
- code_examples: `19`
- has_scripts: `False`
- has_references: `True`
- has_assets: `False`
